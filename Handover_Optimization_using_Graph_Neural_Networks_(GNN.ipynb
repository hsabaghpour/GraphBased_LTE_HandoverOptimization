{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKRnk5S+6wMYwaUu8MM05d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsabaghpour/GraphBased_LTE_HandoverOptimization/blob/main/Handover_Optimization_using_Graph_Neural_Networks_(GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Overview:\n",
        "\n",
        "\t•\tObjective: Optimize the handover success rate in an LTE network by using a Graph Neural Network (GNN) to predict and improve NBR (neighbor relations) between eNodeBs (base stations).\n",
        "\t•\tNodes: eNodeBs represent the base stations in the LTE network.\n",
        "\t•\tEdges: NBR relations (handover success rates) between eNodeBs, weighted by success rate values.\n",
        "\n",
        "We will use PyTorch Geometric, a powerful library that extends PyTorch to work with graphs and GNNs.\n"
      ],
      "metadata": {
        "id": "qUQLavfaDj99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ui6vzXXDV8c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import from_networkx\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t•\ttorch: Standard PyTorch library for defining the GNN and training the model.\n",
        "\t•\tPyTorch Geometric (torch_geometric): A library for deep learning on graphs, which provides GNN layers like GCNConv for graph convolutions.\n",
        "\t•\tNetworkX (nx): Used to create the LTE graph with eNodeBs and NBR relations."
      ],
      "metadata": {
        "id": "HROBHmR9DwCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_data = {\n",
        "    ('eNodeB1', 'eNodeB2'): 0.9,\n",
        "    ('eNodeB1', 'eNodeB3'): 0.85,\n",
        "    ('eNodeB2', 'eNodeB3'): 0.6,\n",
        "    ('eNodeB3', 'eNodeB4'): 0.7,\n",
        "    ('eNodeB1', 'eNodeB4'): 0.4,\n",
        "}"
      ],
      "metadata": {
        "id": "pg7OHZW4DvV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t•\tThe dictionary contains pairs of eNodeBs as edges and their associated handover success rates as weights.\n",
        "\t•\tExample: eNodeB1 and eNodeB2 have a handover success rate of 0.9."
      ],
      "metadata": {
        "id": "XLIrjfhKD29H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t•\tWe initialize a NetworkX graph G and add edges representing NBR relations between eNodeBs.\n",
        "\t•\tThe weight on each edge is the handover success rate."
      ],
      "metadata": {
        "id": "KlqdeZkSD7cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "for (node1, node2), success_rate in graph_data.items():\n",
        "    G.add_edge(node1, node2, weight=success_rate)"
      ],
      "metadata": {
        "id": "7Alk9FqKDsFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert to PyTorch Geometric Graph Format\n",
        "\t•\tfrom_networkx(): Converts the NetworkX graph into a PyTorch Geometric data object that can be used with GNN layers.\n",
        "\t•\tNode Features (data.x): Dummy node features are provided as an example. In a real scenario, node features could represent the characteristics of each eNodeB (e.g., traffic load, signal strength).\n",
        "\t•\tEdge Weights (data.edge_attr): The success rates (weights of edges) are used as features for the edges."
      ],
      "metadata": {
        "id": "7zaQ98n3EA2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = from_networkx(G)\n",
        "data.x = torch.tensor([[0], [1], [1], [1], [0]], dtype=torch.float)  # Example node features\n",
        "data.edge_attr = torch.tensor([list(graph_data.values())], dtype=torch.float).T  # Edge weights as features"
      ],
      "metadata": {
        "id": "ADLSsWYtD-0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the GCN (Graph Convolutional Network) Model**\n",
        "\n",
        "\t•\tThe GCN (Graph Convolutional Network) consists of two convolutional layers (GCNConv):\n",
        "\t•\tconv1: Takes the node features as input and produces a 16-dimensional feature vector for each node.\n",
        "\t•\tconv2: Further transforms the 16-dimensional features into 32 dimensions.\n",
        "\t•\tReLU Activation: Introduces non-linearity to the model.\n",
        "\t•\tLog Softmax: The output is a log probability distribution for each node (or edge)."
      ],
      "metadata": {
        "id": "9Pi_p2dQIMbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 16)  # First GCN layer (input feature size is 1)\n",
        "        self.conv2 = GCNConv(16, 32)  # Second GCN layer (output feature size is 32)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "FHZWYOwZItzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wQ6FEvjeELIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Training Setup**\n",
        "\n",
        "\t•\tModel: The GCN model is instantiated.\n",
        "\t•\tOptimizer: We use the Adam optimizer with a learning rate of 0.01.\n",
        "\t•\tLoss Function: CrossEntropyLoss is used to compare the model’s predictions against the true labels.\n",
        "\n",
        "7. Train the Model"
      ],
      "metadata": {
        "id": "lf_ZjZ71Iw27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "MXZRZjNII40R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t•\tTraining Loop: The model is trained for 100 epochs. In each epoch:\n",
        "\t•\tThe model performs a forward pass on the graph data.\n",
        "\t•\tThe loss is computed between the model’s predictions and the true labels ([0, 1, 1, 1, 0]), representing whether the handover was successful (1) or failed (0).\n",
        "\t•\tThe model’s parameters are updated using backpropagation and the optimizer."
      ],
      "metadata": {
        "id": "KMMFdHwWI8Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)  # Forward pass\n",
        "    loss = criterion(out, torch.tensor([0, 1, 1, 1, 0]))  # Example target\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Update model parameters\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "Pqc71XuJI-2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** Model Prediction**\n",
        "\n",
        "\n",
        "\t•\tAfter training, the model is set to **evaluation mode.**\n",
        "\t•\tThe trained model is used to predict the success of NBR relations based on the learned representations of nodes and edges.\n",
        "\t•\tThe output is a log probability distribution over the classes (success/failure of handovers)."
      ],
      "metadata": {
        "id": "yKWEV5GlJFKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of GNN Approach:\n",
        "\n",
        "\t1.\tGraph Construction: The LTE network is represented as a graph, where nodes are eNodeBs, and edges represent NBR relations with handover success rates as weights.\n",
        "\t2.\tGraph Representation Learning: The GCN model learns node and edge representations through graph convolutions, using node features and edge weights to propagate information across the graph. This helps the model capture the structure of the network and the relationships between eNodeBs.\n",
        "\t3.\tHandover Prediction: After training, the model can predict the success or failure of NBR relations. Successful relations (high handover success rates) are kept, while unsuccessful relations can be removed or adjusted to optimize the network.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "\t•\tGraph Neural Network (GNN): Learns to optimize NBR relations by leveraging both node (eNodeB) features and edge (handover success rate) features.\n",
        "\t•\tGCNConv Layers: These layers aggregate and transform node features by considering the structure of the graph, which allows the model to learn from the relationships between nodes.\n",
        "\t•\tEdge Weights (Handover Success Rates): These weights influence the training process by serving as important features for learning how successful or unsuccessful handovers occur.\n",
        "\n",
        "Outcome:\n",
        "\n",
        "The GNN model provides predictions on which NBR relations are likely to be successful and which may fail. This enables the optimization of the handover process by focusing on strengthening successful relations and removing or adjusting weaker ones."
      ],
      "metadata": {
        "id": "DZa5eoMWJVZd"
      }
    }
  ]
}